{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importation du Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 10:29:39.613578: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-17 10:29:39.667202: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-17 10:29:39.667251: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-17 10:29:39.667280: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-17 10:29:39.676448: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-17 10:29:39.677242: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 10:29:40.995061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'éléments dans l'entraînement: 60000\n",
      "Nombre d'éléments dans le test: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = load_data()\n",
    "print(f'Nombre d\\'éléments dans l\\'entraînement: {len(X_train)}')\n",
    "print(f'Nombre d\\'éléments dans le test: {len(X_test)}')\n",
    "mean = X_train.max() / 2\n",
    "X_train = (np.float32(X_train) - mean) / mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implémentation de notre Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from typing import List, Callable\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Classe pour encapsuler des transformations d'images.\"\"\"\n",
    "    def __init__(self, transforms: List[Callable]):\n",
    "        \"\"\"\n",
    "        Initialise la classe Compose avec une liste de transformations.\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image: Image.Image) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Applique séquentiellement les transformations à une image.\n",
    "        \"\"\"\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random as rd\n",
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Classe Dataset pour un modèle de type GAN.\"\"\"\n",
    "    def __init__(self,  dataset: List[Dict], transform: Optional[Callable]):\n",
    "        \"\"\"\n",
    "        Initialise la classe Dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Obtient un exemplaire de la base de données à partir de son indice et de sa représentation/labellisation tensorielle par le modèle.\n",
    "        \"\"\"\n",
    "        image = Image.fromarray(self.dataset[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Générateur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (caractéristiques, neurones)\n",
    "\"\"\"generator_architecture = [(100, 128),\n",
    "                          'ReLU',\n",
    "                          (128, 256),\n",
    "                          'ReLU',\n",
    "                          (256,512),\n",
    "                          'ReLU',\n",
    "                          (512, 1024),\n",
    "                          'ReLU',\n",
    "                          (1024, 28*28),\n",
    "                          'Tanh']\"\"\"\n",
    "\n",
    "generator_architecture = [(100, 1200),\n",
    "                          'ReLU',\n",
    "                          (1200, 1200),\n",
    "                          'ReLU',\n",
    "                          (1200, 28*28),\n",
    "                          'Tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Générateur d'images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = self._create_network(generator_architecture)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        return self.network(noise)\n",
    "    \n",
    "    def _create_network(self, architecture):\n",
    "        layers = []\n",
    "        for element in architecture:\n",
    "            if type(element) == tuple:\n",
    "                layers.append(nn.Linear(element[0], element[1]))\n",
    "\n",
    "            elif type(element) == str:\n",
    "                try:\n",
    "                    activation_function = getattr(nn, element)\n",
    "                    layers.append(activation_function())\n",
    "                except AttributeError:\n",
    "                    print(f'Fonction d\\'activation non présente dans la classe {nn}: ', element)\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Discriminateur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# (caractéristiques, neurones)\n",
    "discriminator_architecture = [(28*28, 128),\n",
    "                          'LeakyReLU',\n",
    "                          (128, 256),\n",
    "                          'LeakyReLU',\n",
    "                          (256,512),\n",
    "                          'LeakyReLU',\n",
    "                          (512, 512),\n",
    "                          'LeakyReLU',\n",
    "                          (512, 256),\n",
    "                          'LeakyReLU',\n",
    "                          (256, 128),\n",
    "                          'LeakyReLU',\n",
    "                          (128, 1),\n",
    "                          'Sigmoid']\"\"\"\n",
    "\n",
    "# (caractéristiques, neurones)\n",
    "discriminator_architecture = [(28*28, 240),\n",
    "                          'LeakyReLU',\n",
    "                          (240, 240),\n",
    "                          'LeakyReLU',\n",
    "                          (240, 1),  \n",
    "                          'Sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminateur d'images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = self._create_network(discriminator_architecture)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def _create_network(self, architecture):\n",
    "        layers = []\n",
    "        for element in architecture:\n",
    "            if type(element) == tuple:\n",
    "                layers.append(nn.Linear(element[0], element[1]))\n",
    "\n",
    "            elif type(element) == str:\n",
    "                try:\n",
    "                    activation_function = getattr(nn, element)\n",
    "                    layers.append(activation_function())\n",
    "                except AttributeError:\n",
    "                    print(f'Fonction d\\'activation non présente dans la classe {nn}: ', element)\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(batch_size: int, dim):\n",
    "    out = torch.empty(batch_size, dim)\n",
    "    mean = torch.zeros(batch_size, dim)\n",
    "    std = torch.ones(dim)\n",
    "    out = torch.normal(mean, std, out=out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_fn(loader: DataLoader, generator: nn.Module, discriminator: nn.Module, goptimizer: optim, doptimizer:optim, device: str):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    dmean_loss = []\n",
    "    for x in loop:\n",
    "        x = x.to(device)\n",
    "        z = noise(loader.batch_size, dim=100)\n",
    "        z = z.to(device)\n",
    "        f_loss = torch.nn.BCELoss()(discriminator(generator(z)).reshape(loader.batch_size), torch.zeros(loader.batch_size, device=device))\n",
    "        r_loss = torch.nn.BCELoss()(discriminator(x).reshape(loader.batch_size), torch.ones(loader.batch_size, device=device))\n",
    "        loss = (r_loss + f_loss) / 2\n",
    "        doptimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        doptimizer.step()\n",
    "        dmean_loss.append(loss.item())\n",
    "\n",
    "        loop.set_postfix(dloss=loss.item())\n",
    "\n",
    "    dmean_loss = sum(dmean_loss) / len(dmean_loss)\n",
    "\n",
    "    z = noise(loader.batch_size, dim=100).to(device)\n",
    "    g = generator(z)\n",
    "    loss = torch.nn.BCELoss()(discriminator(g).reshape(loader.batch_size), torch.ones(loader.batch_size, device=device))\n",
    "    goptimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    goptimizer.step()\n",
    "\n",
    "    print(g.shape)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(g[0].data.cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    return dmean_loss, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:22<00:00, 26.49it/s, dloss=2.82e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLoss = 0.004451846005219219 | GLoss = 9.818951606750488\n",
      "2/50:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 304/600 [02:30<02:28,  1.99it/s, dloss=1.03e-25]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "# Hyperparamètres Entraînement\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.5\n",
    "MILESTONES = [10]\n",
    "WEIGHT_DECAY = 5e-4\n",
    "BATCH_SIZE = 100\n",
    "SIZE = 28\n",
    "\n",
    "# Hyperparamètres Dataset/Dataloader\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "PIN_MEMORY = True\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    discriminator = Discriminator().to(device)\n",
    "    generator = Generator().to(device)\n",
    "\n",
    "    # Optimiseur\n",
    "    doptimizer = optim.SGD(discriminator.parameters(), lr=LEARNING_RATE, momentum=0.5)\n",
    "    goptimizer = optim.SGD(generator.parameters(), lr=LEARNING_RATE, momentum=0.5)\n",
    "\n",
    "\n",
    "    # Pas variant en fonction des epochs\n",
    "    dscheduler = MultiStepLR(doptimizer, milestones=MILESTONES, gamma=0.1)\n",
    "    gscheduler = MultiStepLR(goptimizer, milestones=MILESTONES, gamma=0.1)\n",
    "\n",
    "    # Transformateur: transforme les images et les vecteurs label_matrix en tenseur\n",
    "    transform = Compose([transforms.Resize((SIZE, SIZE)), transforms.ToTensor(),])\n",
    "    \n",
    "    train_dataset = Dataset(X_train, transform)\n",
    "    loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=SHUFFLE,\n",
    "    drop_last=DROP_LAST)\n",
    "\n",
    "    dmean_loss = []\n",
    "    gmean_loss = []\n",
    "    # Apprentissage\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(f'{epoch}/{EPOCHS}:')\n",
    "        dloss, gloss = train_fn(loader, generator, discriminator, goptimizer, doptimizer, device)\n",
    "        print(f'DLoss = {dloss} | GLoss = {gloss}')\n",
    "        \n",
    "        dmean_loss.append(dloss)\n",
    "        gmean_loss.append(gloss)\n",
    "\n",
    "        dscheduler.step()\n",
    "        gscheduler.step()\n",
    "\n",
    "    plt.plot(dmean_loss)\n",
    "    plt.plot(gmean_loss)\n",
    "    plt.show()\n",
    "\n",
    "    NB_IMAGES = 25\n",
    "    z = noise(NB_IMAGES, dim=100).to(device)\n",
    "    x = generator(z)\n",
    "    plt.figure(figsize=(17, 17))\n",
    "    for i in range(NB_IMAGES):\n",
    "        plt.subplot(5, 5, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(x[i].data.cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
